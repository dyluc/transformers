{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10fab72-1acb-4b5e-8197-04c66b7b9264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from functools import partial\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "# funcs\n",
    "def batch_annotations(annotations, batch_size):\n",
    "    batches = []\n",
    "    for i in range(0, len(annotations), batch_size):\n",
    "        batch = annotations[i:i+batch_size]\n",
    "        file_names, labels = [], []\n",
    "        for example in batch:\n",
    "            file_names.append(example[\"file_name\"])\n",
    "            labels.append(example[\"category_id\"])\n",
    "        \n",
    "        zipped_batch = list(zip(file_names, labels))\n",
    "        random.shuffle(zipped_batch) # shuffle\n",
    "        file_names, labels = map(list, zip(*zipped_batch)) # unpack and convert back to lists\n",
    "        \n",
    "        batches.append((file_names, labels))\n",
    "    return batches\n",
    "\n",
    "def resize_image(file_name, label, directory_prefix, resize_method):\n",
    "    try:\n",
    "        # load and decode jpeg\n",
    "        image = tf.io.read_file(tf.strings.join([tf.constant(directory_prefix), file_name]))\n",
    "        image = tf.io.decode_jpeg(image, channels=3)\n",
    "    except tf.errors.OpError as e:\n",
    "        tf.print(f\"Error loading or decoding image: {file_name}\")\n",
    "        raise e\n",
    "\n",
    "    # resize\n",
    "    if resize_method == \"crop_or_pad\":\n",
    "        image = tf.image.resize_with_crop_or_pad(image, 224, 224)\n",
    "    elif resize_method == \"pad\":\n",
    "        image = tf.image.resize_with_pad(image, 224, 224)\n",
    "    else:\n",
    "        image = tf.image.resize(image, [224, 224])\n",
    "\n",
    "    # inspect value in graph mode (alternatively can turn on eager mode for debugging)\n",
    "    # tf.print(\"Range\", tf.reduce_min(image), \"Max value:\", tf.reduce_max(image))\n",
    "\n",
    "    # encode again for later serializing into TFRecord\n",
    "    image = tf.io.encode_jpeg(tf.cast(image, tf.uint8))\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def check_safe_shuffle(example_batches, categories):\n",
    "    \"\"\"\n",
    "    Ensure correct category IDs after shuffling.\n",
    "    \"\"\"\n",
    "    for example_batch in example_batches:\n",
    "        file_names, labels = example_batch[0], example_batch[1]\n",
    "        file_names_length = len(file_names)\n",
    "        assert file_names_length == len(labels)\n",
    "        for i in range(0, file_names_length):\n",
    "            category_name = categories[str(labels[i])].replace(\"\\u00D7\", \"\") # e.g. id 2061\n",
    "            assert category_name in file_names[i]\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def serialize_example(image, label):\n",
    "    image_shape = image.shape\n",
    "\n",
    "    feature = {\n",
    "        \"image\": _bytes_feature(image),\n",
    "        \"label\": _int64_feature(label.numpy())\n",
    "    }\n",
    "    \n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "\n",
    "def write_compressed_tfrecord(dataset, record_file):\n",
    "    options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "    with tf.io.TFRecordWriter(record_file, options) as writer:\n",
    "        for image, label in dataset:\n",
    "            serialized_example = serialize_example(image, label)\n",
    "            writer.write(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e15bfc1-05c0-4aa4-b495-02b70ff038c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-1-of-22.tfrecord (total time 2.28s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-2-of-22.tfrecord (total time 2.00s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-3-of-22.tfrecord (total time 2.02s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-4-of-22.tfrecord (total time 2.05s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-5-of-22.tfrecord (total time 2.15s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-6-of-22.tfrecord (total time 0.53s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-7-of-22.tfrecord (total time 2.03s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-8-of-22.tfrecord (total time 2.05s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-9-of-22.tfrecord (total time 2.05s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-10-of-22.tfrecord (total time 2.07s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-11-of-22.tfrecord (total time 2.23s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-12-of-22.tfrecord (total time 2.02s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-13-of-22.tfrecord (total time 1.92s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-14-of-22.tfrecord (total time 1.92s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-15-of-22.tfrecord (total time 1.92s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-16-of-22.tfrecord (total time 2.13s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-17-of-22.tfrecord (total time 1.98s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-18-of-22.tfrecord (total time 1.98s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-19-of-22.tfrecord (total time 1.98s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-20-of-22.tfrecord (total time 2.02s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-21-of-22.tfrecord (total time 2.03s)\n",
      "Batch preprocessing complete for /Volumes/t7/train_val_images-processed(Aves)/val2017/inat17_batch-22-of-22.tfrecord (total time 1.98s)\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "records_per_file = 1000 # also acts as batch size / buffer size\n",
    "dataset_type = \"val2017\"\n",
    "directory_prefix = \"/Volumes/t7/\"\n",
    "tfrecord_file_prefix = f\"{directory_prefix}train_val_images-processed(Aves)/{dataset_type}/\"\n",
    "resize_method = \"pad\"\n",
    "\n",
    "# read processed json batch annotations\n",
    "with open(f\"./train_val2017/{dataset_type}-processed.json\", \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "    # create Aves subset\n",
    "    annotations = [anno for anno in annotations if anno['file_name'].startswith('train_val_images/Aves')]\n",
    "\n",
    "    random.shuffle(annotations) # shuffle\n",
    "\n",
    "example_batches = batch_annotations(annotations, records_per_file)\n",
    "example_batches_len = len(example_batches)\n",
    "random.shuffle(example_batches) # shuffle\n",
    "\n",
    "# ensure shuffling correct\n",
    "with open(f\"./train_val2017/categories.json\", \"r\") as f:\n",
    "    categories = json.load(f)\n",
    "check_safe_shuffle(example_batches, categories)\n",
    "\n",
    "# ensure directories exist\n",
    "os.makedirs(os.path.dirname(tfrecord_file_prefix), exist_ok=True)\n",
    "\n",
    "# prepare data, preprocess and shuffle batches\n",
    "dataset_map_func = partial(resize_image, directory_prefix=directory_prefix, resize_method=resize_method)\n",
    "for i, example_batch in enumerate(example_batches):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # construct dataset, resize and shuffle batch\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(example_batch)\n",
    "    dataset = dataset.map(dataset_map_func, num_parallel_calls=tf.data.AUTOTUNE).shuffle(buffer_size=records_per_file)\n",
    "\n",
    "    # write as .tfrecord\n",
    "    tfrecord_file = f\"{tfrecord_file_prefix}inat17_batch-{i+1}-of-{example_batches_len}.tfrecord\"\n",
    "    write_compressed_tfrecord(dataset, tfrecord_file)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Batch preprocessing complete for {tfrecord_file} (total time {total_time:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30778ff8-e19c-43db-8b58-58ae6bc93e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
