{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "[Read the blog post here!](https://dyluc.github.io/2024/09/01/the-transformer-architecture.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How boring! => Qué aburrimiento!\n",
      "I love sports. => Adoro el deporte.\n",
      "Would you like to swap jobs? => Te gustaría que intercambiemos los trabajos?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n",
    "                               extract=True)\n",
    "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text().replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_es = zip(*pairs)\n",
    "for i in range(3):\n",
    "    print(sentences_en[i], \"=>\", sentences_es[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']\n",
      "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']\n"
     ]
    }
   ],
   "source": [
    "vocab_size, max_length = 1000, 50\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])\n",
    "\n",
    "print(text_vec_layer_en.get_vocabulary()[:10])\n",
    "print(text_vec_layer_es.get_vocabulary()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:100_000])\n",
    "X_valid = tf.constant(sentences_en[100_000:])\n",
    "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
    "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
    "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
    "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        \n",
    "        p, i = np.meshgrid(np.arange(max_length), 2 * np.arange(embed_size // 2))\n",
    "        pos_emb = np.empty((1, max_length, embed_size))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
    "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_max_length = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encodings[:, :batch_max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
    "encoder_in = pos_embed_layer(encoder_embeddings)\n",
    "decoder_in = pos_embed_layer(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding and causal masks\n",
    "class PaddingMask(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.math.not_equal(inputs, 0)[:, tf.newaxis]\n",
    "\n",
    "class CausalMask(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        return tf.linalg.band_part(tf.ones((seq_len, seq_len), tf.bool), -1, 0)\n",
    "\n",
    "encoder_pad_mask = PaddingMask()(encoder_input_ids)\n",
    "decoder_pad_mask = PaddingMask()(decoder_input_ids)\n",
    "\n",
    "causal_mask = CausalMask()(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_size, att_heads, ff_units, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=att_heads, key_dim=embed_size, dropout=dropout_rate\n",
    "        )\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_units, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(embed_size),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # multi-head attention sublayer\n",
    "        attn_output = self.attn_layer(inputs, value=inputs, attention_mask=mask)\n",
    "        out1 = self.norm1(tf.keras.layers.Add()([attn_output, inputs]))\n",
    "\n",
    "        # fully connected sublayer\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.norm2(tf.keras.layers.Add()([ffn_output, out1]))\n",
    "\n",
    "        return out2\n",
    "\n",
    "N, att_heads, dropout_rate, ff_units = 2, 8, 0.1, 128\n",
    "encoder_layers = [EncoderLayer(embed_size, att_heads, ff_units, dropout_rate) for _ in range(N)]\n",
    "\n",
    "Z = encoder_in\n",
    "for encoder_layer in encoder_layers:\n",
    "    Z = encoder_layer(Z, mask=encoder_pad_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_size, att_heads, ff_units, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.self_attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=att_heads, key_dim=embed_size, dropout=dropout_rate\n",
    "        )\n",
    "        self.cross_attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=att_heads, key_dim=embed_size, dropout=dropout_rate\n",
    "        )\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.norm3 = tf.keras.layers.LayerNormalization()\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_units, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(embed_size),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, decoder_mask=None, encoder_mask=None):\n",
    "        # self attention sublayer\n",
    "        self_attn_output = self.self_attn_layer(inputs, value=inputs, attention_mask=decoder_mask)\n",
    "        out1 = self.norm1(tf.keras.layers.Add()([self_attn_output, inputs]))\n",
    "\n",
    "        # cross attention sublayer\n",
    "        cross_attn_output = self.cross_attn_layer(out1, value=encoder_outputs, attention_mask=encoder_mask) # use encoder stack final outputs\n",
    "        out2 = self.norm2(tf.keras.layers.Add()([cross_attn_output, out1]))\n",
    "\n",
    "        # fully connected sublayer\n",
    "        ffn_output = self.ffn(out2)\n",
    "        out3 = self.norm3(tf.keras.layers.Add()([ffn_output, out2]))\n",
    "\n",
    "        return out3\n",
    "\n",
    "decoder_layers = [DecoderLayer(embed_size, att_heads, ff_units, dropout_rate) for _ in range(N)]\n",
    "\n",
    "encoder_outputs = Z\n",
    "Z = decoder_in\n",
    "for decoder_layer in decoder_layers:\n",
    "    Z = decoder_layer(Z, encoder_outputs, decoder_mask=causal_mask & decoder_pad_mask, encoder_mask=encoder_pad_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3125/3125 [==============================] - 1441s 460ms/step - loss: 3.1739 - accuracy: 0.3855 - val_loss: 2.3927 - val_accuracy: 0.4910\n",
      "Epoch 2/10\n",
      "3125/3125 [==============================] - 1449s 464ms/step - loss: 2.1355 - accuracy: 0.5303 - val_loss: 1.8105 - val_accuracy: 0.5910\n",
      "Epoch 3/10\n",
      "3125/3125 [==============================] - 1665s 533ms/step - loss: 1.7773 - accuracy: 0.5911 - val_loss: 1.6065 - val_accuracy: 0.6292\n",
      "Epoch 4/10\n",
      "3125/3125 [==============================] - 1612s 516ms/step - loss: 1.6297 - accuracy: 0.6178 - val_loss: 1.5063 - val_accuracy: 0.6472\n",
      "Epoch 5/10\n",
      "3125/3125 [==============================] - 1471s 471ms/step - loss: 1.5442 - accuracy: 0.6331 - val_loss: 1.4597 - val_accuracy: 0.6533\n",
      "Epoch 6/10\n",
      "3125/3125 [==============================] - 1471s 471ms/step - loss: 1.4829 - accuracy: 0.6438 - val_loss: 1.3932 - val_accuracy: 0.6675\n",
      "Epoch 7/10\n",
      "3125/3125 [==============================] - 1481s 474ms/step - loss: 1.4398 - accuracy: 0.6516 - val_loss: 1.3667 - val_accuracy: 0.6728\n",
      "Epoch 8/10\n",
      "3125/3125 [==============================] - 1487s 476ms/step - loss: 1.4049 - accuracy: 0.6587 - val_loss: 1.3442 - val_accuracy: 0.6762\n",
      "Epoch 9/10\n",
      "3125/3125 [==============================] - 1403s 449ms/step - loss: 1.3772 - accuracy: 0.6628 - val_loss: 1.3176 - val_accuracy: 0.6820\n",
      "Epoch 10/10\n",
      "3125/3125 [==============================] - 825s 264ms/step - loss: 1.3522 - accuracy: 0.6674 - val_loss: 1.3312 - val_accuracy: 0.6787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17a7d7280>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10, validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
