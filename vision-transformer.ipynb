{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7fbf95322f5a14b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Vision Transformers\n",
    "\n",
    "1. Reshape input image into sequence of flattened 2d patches.\n",
    "2. Pass flattened patches through trainable linear projection to obtain D dimensions. This output is called the patch embeddings. We will then prepend / concatenate a learnable [class] token to the entire sequence of patch embeddings.\n",
    "4. Add to each patch embedding the positional embedding (element-wise) to represent the patch position in the sequence (learnable 1D embeddings).\n",
    "5. Pass embedded patches through transformer encoder consisting of multiple layers of multi head attention and MLP blocks. Encoder is slightly modified, moving the layer normalization before the multi head self attention and MLP layers (see diagram in paper).\n",
    "6. Pass encoder output for the [class] token embedding to a classification head (feed forward MLP) to obtain the class probabilities (using softmax). Prior to passing through the classification head, the output may first be passed through another normalization layer.\n",
    "\n",
    "The encoder applies self attention across the patch embeddings and the [class] token. The [class] token interacts with all patch embeddings, learning to aggregate global information about the image as the encoder processes the sequence.\n",
    "\n",
    "The encoder output corresponding to the [class] token is fed through an optional normalization layer, and then a classification head (an MLP  using GELU activations). The MLP:\n",
    "- During pre-training: is a two layer feed forward network with one hidden layer and one linear layer (for more expressive learning)\n",
    "- During fine-tuning: is a single linear layer (reducing complexity, to avoid over-fitting on smaller datasets)\n",
    "\n",
    "![vision-transformer](vision-transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef00f3e-5419-489d-bfce-576ba16e46fa",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "We'll take a smaller dataset for data exploration and experimenting with the vision transformer architecture. This dataset is CIFAR-10, a collection of 60,000 color images with 10 classes (6k images per class, images are 32x32 in RGB format). There are 50,000 training and 10,000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7997447-9508-44e9-818a-0d66a63fc259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T08:28:13.016422Z",
     "iopub.status.busy": "2024-11-26T08:28:13.016082Z",
     "iopub.status.idle": "2024-11-26T08:28:13.020408Z",
     "shell.execute_reply": "2024-11-26T08:28:13.019840Z",
     "shell.execute_reply.started": "2024-11-26T08:28:13.016400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265a8a677dceaf32",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-26T08:30:18.417542Z",
     "iopub.status.busy": "2024-11-26T08:30:18.417173Z",
     "iopub.status.idle": "2024-11-26T08:31:01.255200Z",
     "shell.execute_reply": "2024-11-26T08:31:01.254582Z",
     "shell.execute_reply.started": "2024-11-26T08:30:18.417518Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 08:30:19.969336: I external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
      "2024-11-26 08:30:20.055795: I external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
      "2024-11-26 08:30:20.151071: I external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
      "2024-11-26 08:30:20.272570: I external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
      "2024-11-26 08:30:20.458253: I external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
      "2024-11-26 08:30:20.652400: I external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
      "2024-11-26 08:30:20.829875: I external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 162.17 MiB (download: 162.17 MiB, generated: 132.40 MiB, total: 294.58 MiB) to /home/jovyan/tensorflow_datasets/cifar10/3.0.2...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 08:30:20.925912: I external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bec905cd234daf8903fc44352bb6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a97930a24242a9a410d2d5072b679b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454f241e3aaf4df2879f74c04b634b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732609825.565063     159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1732609825.565358     159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1732609825.565500     159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1732609825.678419     159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1732609825.678640     159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1732609825.678789     159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-26 08:30:25.678908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/jovyan/tensorflow_datasets/cifar10/incomplete.KRZVSY_3.0.2/cifar10-train.tfrecord*...:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/jovyan/tensorflow_datasets/cifar10/incomplete.KRZVSY_3.0.2/cifar10-test.tfrecord*...:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset cifar10 downloaded and prepared to /home/jovyan/tensorflow_datasets/cifar10/3.0.2. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 08:31:01.099115: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-11-26 08:31:01.107703: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.environ[\"NO_GCE_CHECK\"] = \"true\"\n",
    "\n",
    "# Load dataset\n",
    "dataset, info = tfds.load(\"cifar10\", with_info=True, as_supervised=True, data_dir=\"~/tensorflow_datasets/\")\n",
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "# CIFAR-10 label names\n",
    "label_names = info.features[\"label\"].int2str\n",
    "\n",
    "# Display a few images, these are stored as image tensors with shape (32, 32, 3) / (height, width, channels)\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, (image, label) in enumerate(train_dataset.take(5)):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(label_names(label))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff5e6a-f9ec-492b-bd0d-d96396e8fc93",
   "metadata": {},
   "source": [
    "# 1. Reshape and Flatten Images \n",
    "\n",
    "This step involves first reshaping each input image into a sequence of flattened 2D patches. The layers will be designed flexibly, handling variable image input sizes and configurable patch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efbf7389-26dd-4f9b-81ca-76d90edeafce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T08:31:14.069542Z",
     "iopub.status.busy": "2024-11-26T08:31:14.069203Z",
     "iopub.status.idle": "2024-11-26T08:31:14.115161Z",
     "shell.execute_reply": "2024-11-26T08:31:14.114641Z",
     "shell.execute_reply.started": "2024-11-26T08:31:14.069519Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class PatchConverter(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0] # get shape of input image tensor and extract batch size\n",
    "        patches = tf.image.extract_patches( # similar to applying a convolution, extracting patches from the image\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1], # sampling rate set to 1 (no dilation)\n",
    "            padding=\"VALID\"\n",
    "        )\n",
    "        patch_dim = patches.shape[-1] # last dimension of patches is the flattened patch size (e.g. 4x4 patches is 48 - 4x4x3)\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dim]) # reshape patches into 3D tensor of shape [batch_size, total patches per image, flattened patch size]\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5530a82b-c35f-4bd5-a528-f9ffd495d441",
   "metadata": {},
   "source": [
    "Using the CIFAR-10 image dimensions (32x32), and a patch size of 4 as an example, we use `tf.image.extract_patches` to extract a grid of 8x8 non-overlapping patches for each image. Each patch contains 4x4x3=48 values. The output here is a 4D tensor of shape [batch_size, 8, 8, 48]. Though the ViT expects a sequence of flattened patches, so `tf.reshape` converts this grid of patches to a sequence with shape [batch_size, 64, 48] to be used for the next step; the patch embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c04eb10-7bd9-471f-a5c2-7ec06bbe6c6f",
   "metadata": {},
   "source": [
    "# 2. Patch Embeddings Through Linear Projection With Class Token\n",
    "\n",
    "The next step in the process is to pass the sequence of flattened patches through a linear projection layer to obtain D-dimensional vectors. These fixed-size embeddings allow the ViT to handle input images of any size. With these patch embeddings, we can prepend a learnable [class] token that will aggregate information from all patches and will be used by the classification head later. \n",
    "\n",
    "Through self-attention, the [class] token will attend to all other patch embeddings as it passes through multiple encoder layers, in the end becoming a global representation of the entire image. The role of the classification head is to properly interpret this representation to predict the class label. The Q, K and V matrices in each attention head determine how each patch attends to other patches in the sequence, and as these are optimized during training through gradient descent, the [class] token learns to better aggregate information from the most relevant patches to improve classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be55290a-4955-43f0-88d2-4ea4f4f70f80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T08:31:19.122288Z",
     "iopub.status.busy": "2024-11-26T08:31:19.121943Z",
     "iopub.status.idle": "2024-11-26T08:31:19.126696Z",
     "shell.execute_reply": "2024-11-26T08:31:19.126018Z",
     "shell.execute_reply.started": "2024-11-26T08:31:19.122262Z"
    }
   },
   "outputs": [],
   "source": [
    "class PatchEmbeddingWithClassToken(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_layer = tf.keras.layers.Dense(embedding_dim) # a simple dense layer for projection\n",
    "        self.class_token = self.add_weight(\n",
    "            shape=(1, 1, embedding_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            name=\"class_token\"\n",
    "        )\n",
    "\n",
    "    def call(self, patches):\n",
    "        patch_embeddings = self.embedding_layer(patches)\n",
    "        batch_size = tf.shape(patch_embeddings)[0]\n",
    "        class_token = tf.broadcast_to(self.class_token, [batch_size, 1, self.class_token.shape[-1]]) # duplicate class_token to match batch size\n",
    "        return tf.concat([class_token, patch_embeddings], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece15d44-a874-46fb-8bbf-f7181e39730f",
   "metadata": {},
   "source": [
    "# 3. Positional Embedding Layer\n",
    "\n",
    "Like Transformers, Vision Transformers don't preserve the inherent position of each patch within the sequence, so we similarly enrich the embeddings with the positional information. These positional embeddings will be learnable, allowing the ViT to self adapt to differing sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500f5b55-17ac-48cb-a446-6499c47ba6a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T08:31:21.952599Z",
     "iopub.status.busy": "2024-11-26T08:31:21.952265Z",
     "iopub.status.idle": "2024-11-26T08:31:21.956546Z",
     "shell.execute_reply": "2024-11-26T08:31:21.955801Z",
     "shell.execute_reply.started": "2024-11-26T08:31:21.952576Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patches, embedding_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = self.add_weight(\n",
    "            shape=(1, num_patches, embedding_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            name=\"positional_embeddings\"\n",
    "        )\n",
    "\n",
    "    def call(self, patch_embeddings):\n",
    "        embeddings = self.position_embeddings + patch_embeddings\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b3f17a-c2d1-4190-b9a5-cb3315262424",
   "metadata": {},
   "source": [
    "# 4. Transformer Encoder Block\n",
    "\n",
    "The encoder of the ViT is very similar to the encoder proposed in the original Transformer paper. The only adjustment involves moving the layer normalization _before_ the multi head self attention and MLP layers. We can easily stack this layer multiple times to create a deep Transformer encoder. We will similarly apply dropout to both the attention mechanism itself, and the final output of the feed forward network for extra regularization.\n",
    "\n",
    "Dropout within the attention mechanism plays a similar role to dropout applied across a fully connected network, it reduces over-reliance on specific portions of the input for improved generalization. This means some of the attention weights are randomly zeroed out, and relationships between the patches (or tokens in the case of regular Transformers) are temporarily ignored during training. Attention is spread across the inputs more evenly, and overfitting is mitigated.\n",
    "\n",
    "> **Masking Note:** Unlike sequence processing tasks typically handled by Transformer architectures, Vision Transformers have no need for padding or causal masks. This is because patch sequence lengths within the image batch are always the same, and they don't have any inherent sequential order, allowing each patch to attend to every other patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73614f89-2165-4409-b35d-9b089aee63dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T08:31:23.839949Z",
     "iopub.status.busy": "2024-11-26T08:31:23.839600Z",
     "iopub.status.idle": "2024-11-26T08:31:23.845563Z",
     "shell.execute_reply": "2024-11-26T08:31:23.844864Z",
     "shell.execute_reply.started": "2024-11-26T08:31:23.839926Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embedding_dim, mlp_dim, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embedding_dim, dropout=dropout_rate\n",
    "        )\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.mlp = tf.keras.Sequential([ # can consider dropout after each of these Dense layers too (try if overfitting)\n",
    "            tf.keras.layers.Dense(mlp_dim, activation=\"gelu\"), # paper user GELU activation\n",
    "            tf.keras.layers.Dense(embedding_dim)\n",
    "        ])\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # mutli-head attention sublayer\n",
    "        x = self.norm1(inputs)\n",
    "        x = self.attn_layer(x, value=x, training=training)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = tf.keras.layers.Add()([x, inputs])\n",
    "        \n",
    "        # mlp sublayer\n",
    "        x1 = self.norm2(x)\n",
    "        x1 = self.mlp(x1)\n",
    "        x1 = self.dropout(x1, training=training)\n",
    "        x1 = tf.keras.layers.Add()([x1, x])\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b12d41-3d0e-4cec-aa27-ba6ac8f4ef40",
   "metadata": {},
   "source": [
    "# 5. Classification Head\n",
    "The class token can now be passed through a classification head to obtain final class probabilities. The paper mentions the following:\n",
    "\n",
    "> \"The classification head is implemented by a MLP with one hidden layer at pre-training\n",
    "time and by a single linear layer at fine-tuning time.\"  \n",
    "> — Dosovitskiy et al. (2020), Vision Transformer (ViT)\n",
    "\n",
    "Though we may not make use of this, the layer can be designed flexibly to easily accommodate for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d00129e1-99c7-43de-bad6-4ef5eba129cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T08:31:27.497936Z",
     "iopub.status.busy": "2024-11-26T08:31:27.497355Z",
     "iopub.status.idle": "2024-11-26T08:31:27.501903Z",
     "shell.execute_reply": "2024-11-26T08:31:27.501275Z",
     "shell.execute_reply.started": "2024-11-26T08:31:27.497912Z"
    }
   },
   "outputs": [],
   "source": [
    "class ClassificationHead(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_classes, mlp_dim=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        output_layer = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "        if mlp_dim:\n",
    "            self.mlp = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(mlp_dim, activation=\"gelu\"),\n",
    "                output_layer\n",
    "            ])\n",
    "        else:\n",
    "            self.mlp = output_layer\n",
    "\n",
    "    def call(self, class_token):\n",
    "        return self.mlp(class_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19d07e-7844-4d98-98fa-bf7f20d942db",
   "metadata": {},
   "source": [
    "# 6. Model Assembly\n",
    "\n",
    "\n",
    "Using these layers, we can construct the Vision Transformer, where input sizes, patch sizes, number of encoder stacks, embedding dimensions are all configurable (allowing flexible use with different datasets and image sizes). A few things to note about the following:\n",
    "\n",
    "- As suggested in the paper, we can apply layer normalization to the class token embedding prior to passing it through the classification head.\n",
    "- In Tensorflow, the shapes are only determined at the point when data is passed through the model `call` method. Therefore, to know the value of `num_patches` for the `PositionalEmbedding` layer, we can rely on the explicit formula (see below). Alternatively, the Tensorflow layer `build` method can initialize the layer weights based on the input size, called the first time the layer processes data. Typical implementation for ViTs assume square images, simplifying patch conversion and calculations for positional encoding (ensure input images are square if using this ViT).\n",
    "\n",
    "$$\\text{num\\_patches} = \\left( \\frac{\\text{image\\_height}}{\\text{patch\\_size}} \\right) \\times \\left( \\frac{\\text{image\\_width}}{\\text{patch\\_size}} \\right) + 1$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75cd8e20-737c-41ea-a1a8-e80a81ea060f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T08:31:31.351454Z",
     "iopub.status.busy": "2024-11-26T08:31:31.351078Z",
     "iopub.status.idle": "2024-11-26T08:31:31.356926Z",
     "shell.execute_reply": "2024-11-26T08:31:31.356189Z",
     "shell.execute_reply.started": "2024-11-26T08:31:31.351431Z"
    }
   },
   "outputs": [],
   "source": [
    "class ViT(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_shape, \n",
    "        patch_size, \n",
    "        num_classes, \n",
    "        embedding_dim, \n",
    "        num_heads, \n",
    "        num_layers, \n",
    "        mlp_dim, \n",
    "        dropout_rate, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_converter = PatchConverter(patch_size)\n",
    "        self.patch_embedding_class_token = PatchEmbeddingWithClassToken(embedding_dim)\n",
    "        self.positional_embedding = PositionalEmbedding(\n",
    "            num_patches=(input_shape[0] // patch_size) ** 2 + 1, # +1 includes [class] embedding\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.encoder_stack = [\n",
    "            EncoderLayer(num_heads, embedding_dim, mlp_dim, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.class_token_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.classification_head = ClassificationHead(num_classes)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Extract the patches and create the patch embeddings w/ class token\n",
    "        patches = self.patch_converter(inputs)\n",
    "        embeddings = self.patch_embedding_class_token(patches)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        embeddings = self.positional_embedding(embeddings)\n",
    "\n",
    "        # Encoder stack\n",
    "        x = embeddings\n",
    "        for layer in self.encoder_stack:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Extract and normalize the [class] token\n",
    "        class_token = x[:, 0]\n",
    "        norm_class_token = self.class_token_norm(class_token)\n",
    "\n",
    "        # Classification head\n",
    "        class_probas = self.classification_head(norm_class_token)\n",
    "        \n",
    "        return class_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504c26e-4c57-4780-b804-6936d30ab8cd",
   "metadata": {},
   "source": [
    "# Layer Output Shape Tests\n",
    "\n",
    "We can cadd some quick tests to ensure the output shape of each layer is correct for a single CIFAR-10 training image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43d33b12-b723-4b33-bd93-553189468b0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T08:31:36.083221Z",
     "iopub.status.busy": "2024-11-26T08:31:36.082885Z",
     "iopub.status.idle": "2024-11-26T08:31:38.661690Z",
     "shell.execute_reply": "2024-11-26T08:31:38.661154Z",
     "shell.execute_reply.started": "2024-11-26T08:31:36.083199Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 08:31:36.133214: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (1, 32, 32, 3), Label: horse\n",
      "Tests passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 08:31:37.702138: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "def test_transformer_layer_output_shapes(image):\n",
    "    input_shape = (32, 32, 3) # image.shape[1:] without batch dim\n",
    "    patch_size = 4\n",
    "    embedding_dim = 64\n",
    "    num_patches = (input_shape[0] // patch_size) ** 2 + 1\n",
    "    num_heads = 4\n",
    "    mlp_dim = 128\n",
    "    dropout_rate = 0.1\n",
    "    num_classes = 10\n",
    "\n",
    "    # Patch Conversion\n",
    "    patch_converter = PatchConverter(patch_size)\n",
    "    patches = patch_converter(single_image)\n",
    "    assert patches.shape == (1, 64, 48)\n",
    "\n",
    "    # Patch Embeddings + Class Token\n",
    "    patch_embedding_with_class_token = PatchEmbeddingWithClassToken(embedding_dim)\n",
    "    embeddings = patch_embedding_with_class_token(patches)\n",
    "    assert embeddings.shape == (1, 65, 64)\n",
    "    assert num_patches == embeddings.shape[1]\n",
    "\n",
    "    # Positional Embeddings\n",
    "    positional_embedding = PositionalEmbedding(num_patches, embedding_dim)\n",
    "    embeddings = positional_embedding(embeddings)\n",
    "    assert embeddings.shape == (1, 65, 64)\n",
    "\n",
    "    # Single Encoder Layer\n",
    "    single_encoder = EncoderLayer(num_heads, embedding_dim, mlp_dim, dropout_rate)\n",
    "    encoder_output = single_encoder(embeddings)\n",
    "    assert encoder_output.shape == (1, 65, 64)\n",
    "    \n",
    "    # Class Token Norm & Classification Head\n",
    "    class_token_norm = tf.keras.layers.LayerNormalization()\n",
    "    classification_head = ClassificationHead(num_classes)\n",
    "    class_token = encoder_output[:, 0]\n",
    "    norm_class_token = class_token_norm(class_token)\n",
    "    class_probas = classification_head(norm_class_token)\n",
    "    assert class_probas.shape == (1, 10)\n",
    "\n",
    "for image, label in train_dataset.take(1):\n",
    "    single_image = tf.expand_dims(image, axis=0) # add a batch dimension\n",
    "    print(f\"Image shape: {single_image.shape}, Label: {label_names(label)}\")\n",
    "    test_transformer_layer_output_shapes(single_image)\n",
    "    print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d0ef0f-6081-4be9-a9e4-0e905abe8cfa",
   "metadata": {},
   "source": [
    "# Data Preprocessing for CIFAR-10\n",
    "\n",
    "Using TFDS, the CIFAR-10 dataset is already split, though we can apply further pre-processing. It's worth noting that Transformer models have fewer inductive biases than ResNets or other CNNs, and so will initially require more training data. CIFAR-10 is a small dataset but it's suited for testing and experimentation, we can later train the model from scratch on larger and larger datasets (CIFAR-100, ImageNet, etc). Further preprocessing we will apply for CIFAR-10:\n",
    "\n",
    "- Normalize / standardize input image.\n",
    "- Split training set further to obtain a validation set.\n",
    "- Shuffle, batch and prefetch.\n",
    "\n",
    "Later we will pre-train on ImageNet and fine tune on CIFAR-10 to observe differences in validation accuracy. To do that, in addition to above, our preprocessing will include:\n",
    "\n",
    "- Apply data augmentation techniques to the datasets.\n",
    "- Resize the CIFAR-10 images to ensure compatibility with architecture and pre-training with ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0ccac5f-80c9-4905-8885-c5322e4a8f90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T08:31:47.897750Z",
     "iopub.status.busy": "2024-11-26T08:31:47.897398Z",
     "iopub.status.idle": "2024-11-26T08:31:48.252663Z",
     "shell.execute_reply": "2024-11-26T08:31:48.252001Z",
     "shell.execute_reply.started": "2024-11-26T08:31:47.897726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images in first batch: (128, 32, 32, 3), with labels: (128,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 08:31:48.244407: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "def normalize_image(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 127.5 - 1.0 # normalize to [-1, 1]\n",
    "    return image, label\n",
    "\n",
    "# load the training, validation and test sets\n",
    "train_set = tfds.load(\"cifar10\", split=\"train[:90%]\", as_supervised=True, data_dir=\"~/tensorflow_datasets/\") # 45,000 examples\n",
    "valid_set = tfds.load(\"cifar10\", split=\"train[90%:]\", as_supervised=True, data_dir=\"~/tensorflow_datasets/\") # 5,000 examples\n",
    "test_set = tfds.load(\"cifar10\", split=\"test\", as_supervised=True, data_dir=\"~/tensorflow_datasets/\") # 10,000 examples\n",
    "\n",
    "# shuffle, batch and prefetch\n",
    "batch_size = 128\n",
    "train_set = train_set.map(normalize_image).shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "valid_set = valid_set.map(normalize_image).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_set = test_set.map(normalize_image).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "for images, labels in train_set.take(1):\n",
    "    print(f\"Shape of images in first batch: {images.shape}, with labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b67ea86f-7319-464b-924d-479825abfcea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T08:36:58.189369Z",
     "iopub.status.busy": "2024-11-26T08:36:58.189025Z",
     "iopub.status.idle": "2024-11-26T08:36:58.230437Z",
     "shell.execute_reply": "2024-11-26T08:36:58.229911Z",
     "shell.execute_reply.started": "2024-11-26T08:36:58.189345Z"
    }
   },
   "outputs": [],
   "source": [
    "# prep callbacks\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"checkpoints/vit_cifar10_saturncloud/vit_cifar10_epoch-{epoch:02d}.weights.h5\",\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# create ViT\n",
    "vit_cifar10_model = ViT(\n",
    "    input_shape=(32, 32, 3),\n",
    "    patch_size=4,\n",
    "    num_classes=10,\n",
    "    embedding_dim=64,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    mlp_dim=128,\n",
    "    dropout_rate=0.1,\n",
    ")\n",
    "\n",
    "# compile\n",
    "vit_cifar10_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0151e55-96ba-4e06-b5fa-2f6f53ab0128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T08:37:04.856745Z",
     "iopub.status.busy": "2024-11-26T08:37:04.856417Z",
     "iopub.status.idle": "2024-11-26T08:41:28.286257Z",
     "shell.execute_reply": "2024-11-26T08:41:28.285705Z",
     "shell.execute_reply.started": "2024-11-26T08:37:04.856722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2793 - loss: 1.9829\n",
      "Epoch 1: val_loss improved from inf to 1.58398, saving model to checkpoints/vit_cifar10_saturncloud/vit_cifar10_epoch-01.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 49ms/step - accuracy: 0.2795 - loss: 1.9825 - val_accuracy: 0.4188 - val_loss: 1.5840\n",
      "Epoch 2/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4484 - loss: 1.5165\n",
      "Epoch 2: val_loss improved from 1.58398 to 1.30431, saving model to checkpoints/vit_cifar10_saturncloud/vit_cifar10_epoch-02.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.4485 - loss: 1.5163 - val_accuracy: 0.5288 - val_loss: 1.3043\n",
      "Epoch 3/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5369 - loss: 1.2803\n",
      "Epoch 3: val_loss improved from 1.30431 to 1.20400, saving model to checkpoints/vit_cifar10_saturncloud/vit_cifar10_epoch-03.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.5370 - loss: 1.2802 - val_accuracy: 0.5546 - val_loss: 1.2040\n",
      "Epoch 4/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5760 - loss: 1.1724\n",
      "Epoch 4: val_loss improved from 1.20400 to 1.15570, saving model to checkpoints/vit_cifar10_saturncloud/vit_cifar10_epoch-04.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.5761 - loss: 1.1723 - val_accuracy: 0.5896 - val_loss: 1.1557\n",
      "Epoch 5/30\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6092 - loss: 1.0831\n",
      "Epoch 5: val_loss improved from 1.15570 to 1.11848, saving model to checkpoints/vit_cifar10_saturncloud/vit_cifar10_epoch-05.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.6092 - loss: 1.0830 - val_accuracy: 0.5994 - val_loss: 1.1185\n",
      "Epoch 6/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6351 - loss: 1.0184\n",
      "Epoch 6: val_loss improved from 1.11848 to 1.08376, saving model to checkpoints/vit_cifar10_saturncloud/vit_cifar10_epoch-06.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.6351 - loss: 1.0184 - val_accuracy: 0.6152 - val_loss: 1.0838\n",
      "Epoch 7/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6590 - loss: 0.9605\n",
      "Epoch 7: val_loss improved from 1.08376 to 1.05186, saving model to checkpoints/vit_cifar10_saturncloud/vit_cifar10_epoch-07.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.6590 - loss: 0.9605 - val_accuracy: 0.6326 - val_loss: 1.0519\n",
      "Epoch 8/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6754 - loss: 0.9088\n",
      "Epoch 8: val_loss improved from 1.05186 to 1.03161, saving model to checkpoints/vit_cifar10_saturncloud/vit_cifar10_epoch-08.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.6754 - loss: 0.9088 - val_accuracy: 0.6396 - val_loss: 1.0316\n",
      "Epoch 9/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6914 - loss: 0.8620\n",
      "Epoch 9: val_loss improved from 1.03161 to 1.03000, saving model to checkpoints/vit_cifar10_saturncloud/vit_cifar10_epoch-09.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.6914 - loss: 0.8620 - val_accuracy: 0.6430 - val_loss: 1.0300\n",
      "Epoch 10/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7092 - loss: 0.8221\n",
      "Epoch 10: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.7092 - loss: 0.8220 - val_accuracy: 0.6436 - val_loss: 1.0333\n",
      "Epoch 11/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7188 - loss: 0.7801\n",
      "Epoch 11: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.7188 - loss: 0.7801 - val_accuracy: 0.6412 - val_loss: 1.0606\n",
      "Epoch 12/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7335 - loss: 0.7518\n",
      "Epoch 12: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.7336 - loss: 0.7518 - val_accuracy: 0.6394 - val_loss: 1.0858\n",
      "Epoch 13/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7459 - loss: 0.7129\n",
      "Epoch 13: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.7459 - loss: 0.7128 - val_accuracy: 0.6474 - val_loss: 1.0759\n",
      "Epoch 14/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7581 - loss: 0.6764\n",
      "Epoch 14: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.7581 - loss: 0.6763 - val_accuracy: 0.6480 - val_loss: 1.0817\n",
      "Epoch 15/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7691 - loss: 0.6446\n",
      "Epoch 15: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.7691 - loss: 0.6445 - val_accuracy: 0.6460 - val_loss: 1.1115\n",
      "Epoch 16/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7861 - loss: 0.6066\n",
      "Epoch 16: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.7861 - loss: 0.6066 - val_accuracy: 0.6494 - val_loss: 1.1225\n",
      "Epoch 17/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7941 - loss: 0.5749\n",
      "Epoch 17: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.7941 - loss: 0.5748 - val_accuracy: 0.6448 - val_loss: 1.1597\n",
      "Epoch 18/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8034 - loss: 0.5515\n",
      "Epoch 18: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.8035 - loss: 0.5515 - val_accuracy: 0.6432 - val_loss: 1.1841\n",
      "Epoch 19/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8186 - loss: 0.5107\n",
      "Epoch 19: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.8186 - loss: 0.5107 - val_accuracy: 0.6418 - val_loss: 1.2231\n",
      "Epoch 20/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8290 - loss: 0.4862\n",
      "Epoch 20: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.8291 - loss: 0.4862 - val_accuracy: 0.6366 - val_loss: 1.2898\n",
      "Epoch 21/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8364 - loss: 0.4585\n",
      "Epoch 21: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.8364 - loss: 0.4585 - val_accuracy: 0.6412 - val_loss: 1.2946\n",
      "Epoch 22/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8441 - loss: 0.4403\n",
      "Epoch 22: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.8441 - loss: 0.4403 - val_accuracy: 0.6436 - val_loss: 1.3161\n",
      "Epoch 23/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8536 - loss: 0.4099\n",
      "Epoch 23: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.8536 - loss: 0.4099 - val_accuracy: 0.6464 - val_loss: 1.3435\n",
      "Epoch 24/30\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8599 - loss: 0.3907\n",
      "Epoch 24: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.8599 - loss: 0.3907 - val_accuracy: 0.6384 - val_loss: 1.3635\n",
      "Epoch 25/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8741 - loss: 0.3553\n",
      "Epoch 25: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.8741 - loss: 0.3553 - val_accuracy: 0.6440 - val_loss: 1.4214\n",
      "Epoch 26/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8773 - loss: 0.3422\n",
      "Epoch 26: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.8773 - loss: 0.3422 - val_accuracy: 0.6368 - val_loss: 1.4532\n",
      "Epoch 27/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8842 - loss: 0.3227\n",
      "Epoch 27: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.8842 - loss: 0.3227 - val_accuracy: 0.6334 - val_loss: 1.4984\n",
      "Epoch 28/30\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8909 - loss: 0.3027\n",
      "Epoch 28: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.8909 - loss: 0.3027 - val_accuracy: 0.6318 - val_loss: 1.5248\n",
      "Epoch 29/30\n",
      "\u001b[1m351/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8976 - loss: 0.2840\n",
      "Epoch 29: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.8976 - loss: 0.2840 - val_accuracy: 0.6304 - val_loss: 1.6130\n",
      "Epoch 30/30\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8991 - loss: 0.2807\n",
      "Epoch 30: val_loss did not improve from 1.03000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.8992 - loss: 0.2807 - val_accuracy: 0.6330 - val_loss: 1.6445\n",
      "Total training time for 30 epochs: 263.43 seconds (4.39 minutes)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# train\n",
    "vit_cifar10_model.fit(\n",
    "    train_set,\n",
    "    validation_data=valid_set,\n",
    "    epochs=30,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total training time for 30 epochs: {total_time:.2f} seconds ({total_time / 60:.2f} minutes)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
