{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e7997447-9508-44e9-818a-0d66a63fc259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "efbf7389-26dd-4f9b-81ca-76d90edeafce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class PatchConverter(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )\n",
    "        patch_dim = patches.shape[-1] # last dimension of patches is the flattened patch size (e.g. 16x16 patches is 768 - 16x16x3)\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dim]) # reshape patches into 3D tensor of shape [batch_size, total patches per image, flattened patch size]\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be55290a-4955-43f0-88d2-4ea4f4f70f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddingWithClassToken(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_layer = tf.keras.layers.Dense(embedding_dim) # a simple dense layer for projection\n",
    "        self.class_token = self.add_weight(\n",
    "            shape=(1, 1, embedding_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            name=\"class_token\"\n",
    "        )\n",
    "\n",
    "    def call(self, patches):\n",
    "        patch_embeddings = self.embedding_layer(patches)\n",
    "        batch_size = tf.shape(patch_embeddings)[0]\n",
    "        class_token = tf.broadcast_to(self.class_token, [batch_size, 1, self.class_token.shape[-1]]) # duplicate class_token to match batch size\n",
    "        return tf.concat([class_token, patch_embeddings], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "500f5b55-17ac-48cb-a446-6499c47ba6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patches, embedding_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = self.add_weight(\n",
    "            shape=(1, num_patches, embedding_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            name=\"positional_embeddings\"\n",
    "        )\n",
    "\n",
    "    def call(self, patch_embeddings):\n",
    "        embeddings = self.position_embeddings + patch_embeddings\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "73614f89-2165-4409-b35d-9b089aee63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embedding_dim, mlp_dim, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attn_layer = tf.keras.layers.MultiHeadAttention( # key_dim = 768 / 12 = 64\n",
    "            num_heads=num_heads, key_dim=embedding_dim // num_heads, dropout=dropout_rate\n",
    "        )\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(mlp_dim, activation=\"gelu\"), # paper user GELU activation\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(embedding_dim)\n",
    "        ])\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # mutli-head attention sublayer\n",
    "        x = self.norm1(inputs)\n",
    "        x = self.attn_layer(x, value=x, training=training)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = tf.keras.layers.Add()([x, inputs])\n",
    "        \n",
    "        # mlp sublayer\n",
    "        x1 = self.norm2(x)\n",
    "        x1 = self.mlp(x1, training=training)\n",
    "        x1 = self.dropout(x1, training=training)\n",
    "        x1 = tf.keras.layers.Add()([x1, x])\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d00129e1-99c7-43de-bad6-4ef5eba129cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_classes, mlp_dim=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        output_layer = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "        if mlp_dim:\n",
    "            self.mlp = tf.keras.Sequential([ # can add dropout here if overfitting during training\n",
    "                tf.keras.layers.Dense(mlp_dim, activation=\"gelu\"),\n",
    "                output_layer\n",
    "            ])\n",
    "        else:\n",
    "            self.mlp = output_layer\n",
    "\n",
    "    def call(self, class_token):\n",
    "        return self.mlp(class_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75cd8e20-737c-41ea-a1a8-e80a81ea060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_shape, \n",
    "        patch_size, \n",
    "        num_classes, \n",
    "        embedding_dim, \n",
    "        num_heads, \n",
    "        num_layers, \n",
    "        mlp_dim, \n",
    "        dropout_rate,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_converter = PatchConverter(patch_size)\n",
    "        self.patch_embedding_class_token = PatchEmbeddingWithClassToken(embedding_dim)\n",
    "        self.positional_embedding = PositionalEmbedding(\n",
    "            num_patches=(input_shape[0] // patch_size) ** 2 + 1, # +1 includes [class] embedding\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.encoder_stack = [\n",
    "            EncoderLayer(num_heads, embedding_dim, mlp_dim, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.class_token_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.classification_head = ClassificationHead(num_classes, 2048)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Extract the patches and create the patch embeddings w/ class token\n",
    "        patches = self.patch_converter(inputs)\n",
    "        embeddings = self.patch_embedding_class_token(patches)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        embeddings = self.positional_embedding(embeddings)\n",
    "\n",
    "        # Encoder stack\n",
    "        x = embeddings\n",
    "        for layer in self.encoder_stack:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Extract and normalize the [class] token\n",
    "        class_token = x[:, 0]\n",
    "        norm_class_token = self.class_token_norm(class_token)\n",
    "\n",
    "        # Classification head\n",
    "        class_probas = self.classification_head(norm_class_token)\n",
    "        \n",
    "        return class_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "431d8b81-34bc-4fcb-999e-2240c62dee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transformer_layer_output_shapes(\n",
    "    single_batch,\n",
    "    input_shape,\n",
    "    patch_size,\n",
    "    embedding_dim,\n",
    "    num_heads,\n",
    "    mlp_dim,\n",
    "    dropout_rate,\n",
    "    num_classes\n",
    "):\n",
    "\n",
    "    batch_size = single_batch.shape[0]\n",
    "    patches_per_image = (input_shape[0] // patch_size) ** 2 \n",
    "    total_patches = patches_per_image + 1 # class token\n",
    "\n",
    "    # Patch Conversion\n",
    "    patch_converter = PatchConverter(patch_size)\n",
    "    patches = patch_converter(single_batch)\n",
    "    assert patches.shape == (batch_size, patches_per_image, embedding_dim)\n",
    "\n",
    "    # Patch Embeddings + Class Token\n",
    "    patch_embedding_with_class_token = PatchEmbeddingWithClassToken(embedding_dim)\n",
    "    embeddings = patch_embedding_with_class_token(patches)\n",
    "    assert embeddings.shape == (batch_size, total_patches, embedding_dim)\n",
    "\n",
    "    # Positional Embeddings\n",
    "    positional_embedding = PositionalEmbedding(total_patches, embedding_dim)\n",
    "    embeddings = positional_embedding(embeddings)\n",
    "    assert embeddings.shape == (batch_size, total_patches, embedding_dim)\n",
    "\n",
    "    # Single Encoder Layer\n",
    "    single_encoder = EncoderLayer(num_heads, embedding_dim, mlp_dim, dropout_rate)\n",
    "    encoder_output = single_encoder(embeddings)\n",
    "    assert encoder_output.shape == (batch_size, total_patches, embedding_dim)\n",
    "    \n",
    "    # Class Token Norm & Classification Head\n",
    "    class_token_norm = tf.keras.layers.LayerNormalization()\n",
    "    classification_head = ClassificationHead(num_classes)\n",
    "    class_token = encoder_output[:, 0]\n",
    "    norm_class_token = class_token_norm(class_token)\n",
    "    class_probas = classification_head(norm_class_token)\n",
    "    assert class_probas.shape == (batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d0ef0f-6081-4be9-a9e4-0e905abe8cfa",
   "metadata": {},
   "source": [
    "# Data Preprocessing for iNat2017\n",
    "\n",
    "See [/dataset-utils](./dataset-utils/README.md) for details on preprocessing approach for this dataset, no augmentation has been applied. Configurations used (inspired in part by values from paper):\n",
    "\n",
    "Model Config:\n",
    "- Image size: `224x224`\n",
    "- Patch size: `16x16`\n",
    "- Embedding dimension: `768`\n",
    "- Number of heads: `12` (64-dim embedding per head)\n",
    "- Encoder stack: `12`\n",
    "- MLP dimension: `3072`\n",
    "- Dropout rate: `0.1`\n",
    "- Classification head (dense layer): `2048` units (with gelu)\n",
    "- Classification head (output layer): `5089` classes (with softmax)\n",
    "\n",
    "Training Config:\n",
    "- Batch size: `32`\n",
    "- Base learning rate: `3e-3` (linear warmup for 10%, then cosine decay)\n",
    "- Optimizer: `AdamW`\n",
    "- Weight decay: `0.1`\n",
    "- Epochs: `30`\n",
    "- Shuffle buffer size: `50000`\n",
    "\n",
    "Roughly 97.8M trainable parameters.\n",
    "\n",
    "Additional:\n",
    "- SageMaker instance size for training: `G5` or `P3` family of instances.\n",
    "- SageMaker storage container: `20GB` (~18GB of processed training and validation data)\n",
    "\n",
    "---\n",
    "\n",
    "The following is a scaled down version of the above configuration. This can still achieve meaningful pretraining results though, and you'll pay much less in cloud compute costs.\n",
    "\n",
    "Model Config:\n",
    "- Embedding dimension: `256`\n",
    "- Number of heads: `4` (64-dim embedding per head)\n",
    "- Encoder stack: `4`\n",
    "- MLP dimension: `1024`\n",
    "- Classification head (dense layer): `512` units\n",
    "\n",
    "Training Config:\n",
    "- Batch size: `16`\n",
    "- Base learning rate: `1e-4`\n",
    "- Epochs: `8`\n",
    "- Shuffle buffer size: `10000`\n",
    "- Dataset subset: `20%`\n",
    "\n",
    "Additional:\n",
    "- SageMaker instance size for training: `G4dn` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0aa0fa8-a76a-4998-9651-5671249f493e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://inat17-train-val-records/train_val_images-processed/val2017/inat17_batch-1-of-96.tfrecord to train_val_images-processed/val2017/inat17_batch-1-of-96.tfrecord\n"
     ]
    }
   ],
   "source": [
    "# validation set\n",
    "!aws s3 cp s3://inat17-train-val-records/train_val_images-processed/val2017/ ./train_val_images-processed/val2017/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25b0efab-592b-45ea-8d68-41e18b9cd5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://inat17-train-val-records/train_val_images-processed/train2017/inat17_batch-1-of-580.tfrecord to train_val_images-processed/train2017/inat17_batch-1-of-580.tfrecord\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "!aws s3 cp s3://inat17-train-val-records/train_val_images-processed/train2017/ ./train_val_images-processed/train2017/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a0ccac5f-80c9-4905-8885-c5322e4a8f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images in first batch: (64, 224, 224, 3), with labels: (64,)\n",
      "Tests passed: test_transformer_layer_output_shapes\n"
     ]
    }
   ],
   "source": [
    "feature_description = {\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"label\": tf.io.FixedLenFeature([], tf.int64)\n",
    "}\n",
    "\n",
    "def normalize_image(image):\n",
    "    return tf.cast(image, tf.float32) / 127.5 - 1.0 # normalize to [-1, 1]\n",
    "\n",
    "def parse_example_safely(serialized_example):\n",
    "    try:\n",
    "        parsed_example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    \n",
    "        # decode and normalize image, and extract label\n",
    "        image = tf.image.decode_jpeg(parsed_example[\"image\"], channels=3)\n",
    "        image = normalize_image(image)\n",
    "        label = parsed_example[\"label\"]\n",
    "        \n",
    "        return image, label\n",
    "    except tf.errors.InvalidArgumentError as e:\n",
    "        tf.print(f\"Error parsing example: {e}\")\n",
    "        return None, None\n",
    "        \n",
    "# create training and validation sets\n",
    "train_set_filepaths = [f\"./train_val_images-processed/train2017/inat17_batch-{i}-of-580.tfrecord\" for i in range(1, 581)]\n",
    "valid_set_filepaths = [f\"./train_val_images-processed/val2017/inat17_batch-{i}-of-96.tfrecord\" for i in range(1, 97)]\n",
    "\n",
    "dataset_buffer_size = 16 * 1024 * 1024\n",
    "train_set = tf.data.TFRecordDataset(\n",
    "    train_set_filepaths, \n",
    "    compression_type=\"GZIP\",\n",
    "    buffer_size=dataset_buffer_size\n",
    ") # 579,184 examples\n",
    "\n",
    "valid_set = tf.data.TFRecordDataset(\n",
    "    valid_set_filepaths, \n",
    "    compression_type=\"GZIP\",\n",
    "    buffer_size=dataset_buffer_size\n",
    ") #  95,986 examples\n",
    "\n",
    "# map, shuffle, batch and prefetch\n",
    "batch_size = 64\n",
    "buffer_size = 50000\n",
    "train_set = (\n",
    "    train_set\n",
    "    .map(parse_example_safely, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .filter(lambda image, label: image is not None and label is not None)\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "valid_set = (\n",
    "    valid_set\n",
    "    .map(parse_example_safely, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .filter(lambda image, label: image is not None and label is not None)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "for image_batch, label_batch in train_set.take(1):\n",
    "    print(f\"Shape of images in first batch: {image.shape}, with labels: {label.shape}\")\n",
    "    test_transformer_layer_output_shapes(\n",
    "        single_batch=image_batch,\n",
    "        input_shape=(224, 224, 3),\n",
    "        patch_size=16,\n",
    "        embedding_dim=768,\n",
    "        num_heads=12,\n",
    "        mlp_dim=3072,\n",
    "        dropout_rate=0.1,\n",
    "        num_classes=5089,\n",
    "    )\n",
    "    print(\"Tests passed: test_transformer_layer_output_shapes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b67ea86f-7319-464b-924d-479825abfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep callbacks\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"checkpoints/vit_inat17_sagemaker/vit_inat17_epoch-{epoch:02d}.weights.h5\",\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "train_set_size = 579184\n",
    "epochs = 30\n",
    "linear_lr_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=3e-3,\n",
    "    decay_steps=(train_set_size // batch_size) * epochs, # base LR * (1 - t / T)\n",
    "    end_learning_rate=0.0,\n",
    ")\n",
    "\n",
    "# create ViT\n",
    "vit_pretraining_model = ViT(\n",
    "    input_shape=(224, 224, 3),\n",
    "    patch_size=16,\n",
    "    num_classes=5089,\n",
    "    embedding_dim=768,\n",
    "    num_heads=12,\n",
    "    num_layers=12,\n",
    "    mlp_dim=3072,\n",
    "    dropout_rate=0.1,\n",
    ")\n",
    "\n",
    "# compile\n",
    "vit_pretraining_model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW( # with weight decay (like l2 regularization)\n",
    "        learning_rate=linear_lr_decay, # we can reduce base lr or weight decay if unstable during training\n",
    "        weight_decay=0.1\n",
    "    ),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0151e55-96ba-4e06-b5fa-2f6f53ab0128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16/Unknown \u001b[1m858s\u001b[0m 53s/step - accuracy: 0.0035 - loss: 8.7261"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/transformers/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 8.76769, saving model to checkpoints/vit_inat17_sagemaker/vit_inat17_epoch-01.weights.h5\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1029s\u001b[0m 64s/step - accuracy: 0.0036 - loss: 8.7212 - val_accuracy: 0.0010 - val_loss: 8.7677\n",
      "Total training time for 1 epochs: 1028.95 seconds (17.15 minutes)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# train\n",
    "vit_pretraining_model.fit(\n",
    "    train_set,\n",
    "    validation_data=valid_set,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total training time for {epochs} epochs: {total_time:.2f} seconds ({total_time / 60:.2f} minutes)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
